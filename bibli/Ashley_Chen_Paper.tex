\documentclass[10pt, conference]{IEEEtran}
\usepackage{booktabs}
\usepackage{mdframed,graphicx,booktabs,textcomp,multirow,pgfplots,cite,booktabs,tikz}
\usepackage{graphicx}

\definecolor{bblue}{HTML}{4F81BD}
\definecolor{rred}{HTML}{C0504D}
\definecolor{ggreen}{HTML}{9BBB59}
\definecolor{ppurple}{HTML}{9F4C7C}

\pgfplotsset{
    integral segments/.code={\pgfmathsetmacro\integralsegments{#1}},
    integral segments=3,
    integral/.style args={#1:#2}{
        ybar interval,
        domain=#1+((#2-#1)/\integralsegments)/2:#2+((#2-#1)/\integralsegments)/2,
        samples=\integralsegments+1,
        x filter/.code=\pgfmathparse{\pgfmathresult-((#2-#1)/\integralsegments)/2}
    }
}


\ifCLASSINFOpdf
 
\else
\fi

\hyphenation{Bug Fix Verification An Analysis of Pull Request Acceptance on the Tree-Structured Level}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Bug Fix Verification: An Analysis of Pull Request Acceptance on the Tree-Structured Level}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Yu-Ann Chen}
\IEEEauthorblockA{Institute of Software Research\\Carnegie Mellon University\\yuannc@andrew.cmu.edu}
}
\maketitle


\begin{abstract}
As society becomes more dependent on Open Source Software (OSS), it becomes more important to understand how we fix bugs. Pull requests are patch proposals to OSS and are accepted if it contributes to the OSS project. But is there a way to understand the process of change and pull request acceptance in OSS? To tackle this problem, We wanted to understand what kind of structural data behavior is present in accepted pull requests, and ultimately, how this behavior can be used to classify and predict pull request acceptance. We collected pull request information from Github's existing pull request system and processed file changes through our analysis. In the recent work around bug identification, we used what is known as naturalness and Change Distiller to mine more information about change in these files. As a result, two primary models were built, one using basic pull request information and the other using data mined from naturalness and Change Distiller. The advanced model reached an overall 64\% accuracy rate at predicting pull request acceptance, which exceeds the basic model's accuracy rate. This approach has proven that structural analysis can be used to understand behavior of pull request acceptance. Ultimately, this work can be used to further understand what kind of bugs can be fixed automatically in OSS and how we can predict the evolution of OSS contribution.
\end{abstract}
\IEEEpeerreviewmaketitle

\section{Introduction}
Bugs have always posed a problem to the software we use. As OSS become more prevalent in society, we are faced to trusting thousands of developers around the world to build software that is dependable by major companies and governments. 

The collaborative coding environment which OSS provides has lowered the barrier of entry for contributors~\cite{Yu}. Git, a version control system for software projects, has allowed developers to fork projects and update them locally. This allows developers to submit their local commits to the main branch of the repository. This is called a pull request. This process for patch submission has made social computing into a more formalized system. As contributors are able to track multiple changes throughout a distributed project, this collaborative environment has made it easier to integrate track issues and patches.

It is obvious that bugs pose a huge problem in the modern practice of software development  [reference]. More importantly, the effects of bugs in OSS can huge impact thousands of users [reference], [reference] and increase the social cost of software [reference]. Therefore, bug management plays a major role in the design of the software itself and in its evolution. In OSS, pull requests act as patch proposals and are submitted to the core developers of the repository. However, 20-25\% of pull requests are reverted [insert reference]. In a repository, more pull requests are rejected than accepted [insert reference] while each OSS can report up to 000 bugs without patches. These claims strongly indicate that developers are unable to fully determine whether a patch actually fixes a bug.

Recent studies have demonstrated techniques to understand changes in source code. Ray et al. [reference] shows that buggy code can be identified by measuring the source code?s naturalness. Furthermore, Gall et al. [reference] created a technique that extracts fine-grained changes from the tree-structured level of code [reference]. We hope to apply these techniques of understanding changes to assessing pull requests in OSS.

We use the information processed by naturalness and Change Distiller to find a correlation in pull request acceptance. We hypothesize that we can build a model that predicts whether a pull request would be accepted or rejected based on the changes made. We speculate that doing so will contribute further findings about the process of fixing bugs in OSS.

We evaluate the approach by computing prediction accuracy and cost effectiveness. This results will help us gain insights into the importance of the information provided on OSS service sites about pull requests. We also evaluate the performance metrics used in the approach by comparing its influence over the model?s prediction accuracy. 

We want to answer the following research questions:
\begin{mdframed} 
\begin{itemize}
\item \textbf{RQ1.} What structural data behavior is present in accepted pull requests?
\item \textbf{RQ2.} How can this behavior be used to classify and predict pull request acceptance?
\item \textbf{RQ3.} What is the most useful metric for predicting pull request acceptance?
\end{itemize}
\end{mdframed}

The rest of this paper is organized as follows: 
Section II provides details on the techniques we are using in our prediction model. Section III and IV describes our approach towards collecting, extracting, and evaluating our data. In Section V, we present the results of our findings and address the research questions from Section I. We draw our conclusions in Section VI and discuss threats to validity and the future work of this paper.

\section{Background}
In this section, we will discuss background information, which will help readers understand what we are trying to do.

\subsection{Pull Request}
A pull request is a form of patch proposal to a OSS repository. A developer forks the repository and eventually wants to merge his/her changes to the master branch. The developer submits the commits with the patch(es) to the repository's owner along with a commit message. The owner than can see the files changed in the repository as well as the specific source code lines changed. The owner can then merge the pull request, which implies that the pull request was accepted and fixed the bug. The owner can also closed the request which indicates that the pull request was rejected.

\subsection{Github}
Github is a web-based version of git for developers to store projects (repositories). Developers are able to use its pull request feature to submit bugs and patches. Github is able to rank its most mature repositories by programming languages. This is evaluated by the amount of forks and stars each repository possess. 

%\graphicxpath{{FlowChart.jpg/}}
%\includegraphics[scale=1]

\subsection{Naturalness}
In [insert Naturalness paper], the use of entropy and cross-entropy of a file could be used to identify buggy code. Entropy, the measure of repetitiveness and predictability is the output of analyzing source code. This is done using a language n-gram model which tracks the repetitiveness of tokens in source code.  Cross-entropy is the measure of entropy over the code?s size (tokens).

\subsection{Change-Distiller}
Change Distiller mines fine-grained changes in source code at the tree-structured level. By taking two versions of a Java file, Change Distiller converts source code into Abstract Syntax Trees (AST). Using a tree-differencing algorithm [insert reference], the difference between the two ASTs can be extracted and classified to types and significant levels (significant level meaning the relevance of the change to the file). 

\subsection{Tree-Structured Behavior}
Tree Structured Behavior is a data mined from analyzing source code at the AST level. This means breaking up a source code into a tree and classifying the data into information which can be put into our predictive model.

\section{Approach}
This section, we will be discussing what approach we took. We first looked at the models that could be built. Then, we looked  at the attributes that can be collected as performance metrics. Then, we collect pull request information from Github. We then run create multiple models for our evaluation.

\subsection{Building Models}
We used Weka, a model building application to process each file in a 10-fold cross validation. This classification process partitions the original dataset into ten subsamples. Nine subsamples will be used for training and one subsample will be used for testing. Ultimately, the 10-fold cross validation reduces the sensitivity of the model?s performance to new data. By having Weka distribute the data for both the training set and the testing of the model, we will not receive surprising results when it makes new predictions for data it has never seen.

\subsection{Data-Analysis}
Using the model built by Weka, We create a confusion matrix to calculate precision, recall, and the AUC. This analysis can then be used to determine calculate the model?s prediction accuracy.

\subsection{Process Changes}
We used a language model called CodeMining-TreeLM to generate entropy level. Because all pull requests are from thirteen different repositories, we trained 13 models with each repositories? source code at an iteration of 400 times. Pull request?s files were matched with their respective project name, and using the serialized training set that was created by CodeMining-TreeLM, each file received an entropy and cross-entropy value.

To process files through Change Distiller, each file was given to Change Distiller in two parts: the before version and the after version of the pull request. Change Distiller then extracts changes and gives a sum of each change type from the file. 

The data mined with Naturalness and Change Distiller are saved in a text file that is specific to each pull requests. For example, a text file for Pull-Request-A may have information for 50 files because the pull request committed 50 files. The number of changes per file will be taken into account in the next step.

\subsection{Filtering Performance Metrics}
We created a script that parses through the text files of each pull request. The script selects the attributes it wants, takes the data, work out some calculation if needed, then formats it into a file which will be the input for building a model. To answer our research questions, we will be adjusting the metrics used from each database. This is why filtering the performance metrics is important to this approach.

\begin{table*}[t]
  \centering
  \caption{List of Attributes (Metrics of Change)}
  \label{tab:Figure I}
  \begin{tabular}{l|rlllr}
    \toprule
    \textbf{Attribute Groups} & \textbf{Number of Attributes} & \textbf{Origin} & \textbf{Level} & \textbf{Format} & \textbf{Range}\\ %colum names
    \midrule
    Number of Files & 1 & Github & Pull Request & Sum & 5,731 \\ %added row
    Lines of Code & 4 & Github & File & Sum, Average & 24,882 \\ %added row
    Files Changed & 4 & Github & Pull Request & Sum & 300 \\ %added row
    Naturalness & 8 & Naturalness & File & Sum, Average & 1.01\\ %added row
    Significant Levels & 6 & Change Distiller & File & Sum, Average &130,680 \\ %added row
     Structural Changes & 148 & Change Distiller & File & Sum, Average & 7,798 \\ %added row
    \bottomrule
  \end{tabular}
  \break
  \newline
\textbf{Table I:} This figure shows the attributes that are extracted from pull requests and their files. The origin describes where each attribute is derived from. The level depicts where the attribute is measured. The format describes how the attribute is interpreted for each pull request.
\end{table*}

\subsection{Data Collection}
We figure~\ref{tab:Figure I} requested and collected the source code from each file?s rawURL using Github?s API. If a pull request was closed and merged, it would be classified as accepted. If a pull was closed and not merged, it would be classified as rejected. Our research questions only pertain to the technical and structure information of pull requests, which means we do not examine or collect information about the developers or the pull request?s comments. The pull requests we collect also can only be Java files, due to the nature of Change Distiller. Therefore, the dataset collected were also filtered to not include pull requests containing non-java files.

\section{Evaluation}
In this section, we will be reviewing the actual evaluations of the approach we took.

\subsection{Validity of Data}
The pull requests collected are validated based on the repository?s maturity. We collected these pull requests from the top 13 Java projects with the most forks, stars, and pull requests. We successfully collected 3,000 pull requests with a balanced ratio of accepted to rejected pull requests of 55 to 45.

\subsection{Predictability Accuracy Comparison}
We apply our approach to the dataset we have collected and built two models: The basic model and advanced model. The basic model encompasses only the attributes from Github, and the advanced model has a combination of the basic model?s attributes, naturalness, and structural changes. Since the basic model serves the baseline for this evaluation, the accuracy of the advanced model will be compared to the basic model. 

\begin{table}[h!]
  \centering
  \caption{Accuracy of Models}
  \label{tab:FigureII}
  \begin{tabular}{lrrr}
    \toprule
    \textbf {Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall}\\ %colum names
    \midrule
    Coin Toss & 00\% & 0.0 & 0.0\\ %added row
    Basic & 58\% & 0.58 & 0.77\\ %added row
    Advanced & 70\% & 0.72 & 0.91\\ %added row
    Basic + Advanced & 69\% & 0.64 & 0.68\\ %added row
    \bottomrule
  \end{tabular}
  \break
  \break
\textbf{Table \ref{tab:FigureII}:} Each model was measured by how accurate it was able to correctly classify pull requests. Precision and recall are also used to evaluate the performance of each model.
\end{table}

\subsection{Validity of Models}
To validate the classifying models used, We took five different types of classifier and measured its performance over the AUC. We took the same data from the advanced model and implemented different algorithms along with Random Forest: LMT, PART, Naive Bayes, and ZeroR. We then compare both the accuracy and the ROC Area.

\begin{table}[h!]
  \centering
  \caption{Performance of Classifiers}
  \label{tab:Figure II}
  \begin{tabular}{c|lr}
    \hline
    \textbf {Dataset} & \textbf{Classifer} & \textbf{ROC Area}\\
    \hline
    \multirow{5}{*}{Basic [6]}
    				& \multicolumn{1}{l}{Random Forest} & \multicolumn{1}{r}{0.70} \\\cline{2-3}
                                 & \multicolumn{1}{l}{LMT} & \multicolumn{1}{r}{0.66} \\\cline{2-3}
                                 & \multicolumn{1}{l}{PART} & \multicolumn{1}{r}{0.61} \\\cline{2-3}
                                 & \multicolumn{1}{l}{Naive Bayes} & \multicolumn{1}{r}{0.53} \\\cline{2-3}
                                 & \multicolumn{1}{l}{ZeroR} & \multicolumn{1}{r}{0.50} \\\hline
    \multirow{5}{*}{Advanced [74]} 
    				& \multicolumn{1}{l}{Random Forest} & \multicolumn{1}{r}{0.00} \\\cline{2-3}
                                 & \multicolumn{1}{l}{LMT} & \multicolumn{1}{r}{0.00} \\\cline{2-3}
                                 & \multicolumn{1}{l}{PART} & \multicolumn{1}{r}{0.00} \\\cline{2-3}
                                 & \multicolumn{1}{l}{Naive Bayes} & \multicolumn{1}{r}{0.00} \\\cline{2-3}
                                 & \multicolumn{1}{l}{ZeroR} & \multicolumn{1}{r}{0.00} \\\hline
  \end{tabular}
  \break
  \break
\textbf{Table III:} Each model was measured by how accurate it was able to correctly classify pull requests. Precision and recall are also used to evaluate the performance of each model.
\end{table}

\begin{figure}  
\begin{center}
\textbf{Your title}\par\medskip \break
\begin{tikzpicture}[color=black,/pgf/declare function={f=3*e^(-x)*x^3;}]
\begin{axis}[
    domain=0:8.1,
    samples=100,
    axis lines=middle
]
\addplot [ultra thick] {f};
\addplot [
    white,
    integral segments=4,
    integral=0:8
] {f};
\end{axis}
\end{tikzpicture}
\newline \small \textbf{Table II:} Each model was measured by how accurate it was able to correctly classify pull requests. Precision and recall are also used to evaluate the performance of each model.
\end{center}
\end{figure}


\section{Discussion of Results}
\subsection{Research Questions \#1}
Table I provides a list of attributes that are found in pull requests. *We will expand on this answer*

\subsection{Research Question \#2}
We show that fine-grained source code changes in pull request can lead to improved models for predicting pull request acceptance. In Table II, the basic model received an accuracy rate of 58\% and the advanced model generated a 69\% accuracy rate. With the addition of the behavior presented in Research Question 1, the model was able to predict at a 11\% higher accuracy. We further evaluated the performance of the models by comparing their precision and recall. The advanced model had a 0.14 higher precision and recall. Based on these results, we can answer the question: The structured behavior of accepted pull requests can predict pull request acceptance by creating a model using the Random Forest algorithm.

*We will talk about the results of evaluating the different machine-learning algorithms used in this research*

\begin{table}[h!]
  \centering
  \caption{ Usefulness of Metrics}
  \label{tab:Figure VI}
  \begin{tabular}{l|lrr}
    \toprule
    \textbf {Metric} & \textbf{Type} & \textbf{Accuracy} & \textbf{Difference}\\ %colum names
    \midrule
    \# of Files & Basic & 67\% & +0.4\%\\ %added row
    Method Declared & Advanced & 67\% & +0.4\%\\ %added row
   Entropy-A & Advanced & 67\% & +0.4\%\\%added row
    Return Statement & Advanced & 67\% & +0.4\%\\ %added row
    \bottomrule
  \end{tabular}
  \break
  \break
\textbf{Table IV:} Each metric used in this paper is presented with an accuracy of a predictive model without the metric. The difference of each metric shows the increase/decrease in accuracy when it is missing from the advanced model. This table shows the influence of each metric over the predictive model.
\end{table}

\begin{table}[h!]
  \centering
  \caption{ Usefulness of Metrics (Grouped)}
  \label{tab:Figure VII}
  \begin{tabular}{l|rllrr}
    \toprule
    \textbf {Metric} & \textbf{\# of Metrics} & \textbf{Type} & \textbf{Accuracy} & \textbf{Difference}\\% [5ex]%colum names
    \midrule
    \# of Files & 6 & Basic & 67\% & +0.4\%\\ %added row
    Method Declared & 4 & Advanced & 67\% & +0.4\%\\ %added row
   Entropy-A & 74 & Advanced & 67\% & +0.4\%\\%added row
    \bottomrule
  \end{tabular}
  \break
  \break
\textbf{Table V:} Overall influence of each group of metrics. 
\end{table}

\subsection{Research Question \# 3}
In our approach, we created various models with the same data but with a unique metric missing. Taking the accuracy of each model, we are able to measure the influence which each metric has on the overall advance model?s accuracy. In our findings, we discovered that [insert how many metrics out of total metrics] presented by the basic model only decreased the overall accuracy rate. This information is shown in Table VI. As a result of removing these metrics, the accuracy increased from 70\% to 69\%. Beyond the metrics found in the basic model, our findings also showed that the entropy metrics and structural changes had the biggest influence over the accuracy rate. The model which excluded entropy metrics had the biggest decrease in accuracy rate of 4\%. We pose that the ?usefulness? of a metric is defined by its positive influence over the overall accuracy. We can now answer the third research question: Entropy and structural changes had the most influence over the accuracy of the advanced model.


\begin{tikzpicture}
    \begin{axis}[
        width  = 8cm,%*\textwidth,
        height = 5cm,
        major x tick style = transparent,
        ybar=2*\pgflinewidth,
        bar width=14pt,
        ymajorgrids = true,
        ylabel = {Percent in Change (\%)},
        symbolic x coords={Basics,Entropy,Structural Changes},
        xtick = data,
        scaled y ticks = false,
        enlarge x limits=0.25,
        ymin=0,
        legend cell align=left,
        legend style={
                at={(1,1.05)},
                anchor=south east,
                column sep=1ex
        }
    ]
        \addplot[style={bblue,fill=bblue,mark=none}]
            coordinates {(Basics, 1.0) (Entropy,1.0) (Structural Changes,1.0)};

        \addplot[style={rred,fill=rred,mark=none}]
             coordinates {(Basics,1.123) (Entropy,0.85) (Structural Changes,1.09)};

        \legend{Accuracy,Difference}
    \end{axis}
    \end{tikzpicture}
\newline \small \textbf{Table II:} The accuracy bars compare the accuracy rate of each model missing the group of metrics (see Figure VII for a table version of this figure). The metric grouping with the  least accurate model in this graph should be interpreted as the most useful metrics group. The difference bar shows the increase and decrease of the prediction accuracy compared to the advanced model. Metric groups with positive difference bars had negative influences over the model (this is vice versa for negative difference bars).


\section{Conclusion}
Throughout this paper, we analyzed what change occurs in pull requests. The following are the results we have concluded:

\begin{mdframed} 
\begin{itemize}
\item \textbf{RQ1.} High entropy and structural changes are present in accepted pull requests.
\item \textbf{RQ2.} The model with the these behavior as performance metrics performed better with a higher accuracy rate.
\item \textbf{RQ3.} Entropy and structural changes are also the biggest influence in the model.
\end{itemize}
\end{mdframed}

Overall, the nature of this project contributes findings to understanding the nature of social computing, specifically OSS. 

\subsection{Threats to Validity}
This project faces one particular threat to its validity. In the nature of bug fixing and the definition of a bug, a pull request acceptance may not indicate whether a pull request fixed a bug. This could alter the collected data and detour from the high-level purpose of the project. However, since the advanced model proved to have a high predicting accuracy, this threat may not as be detrimental as we thought. Additional threat to  our data?s validity includes our limiting dataset. It is evident that Java projects are more explicit with its data structure than other programming languages. In terms of practicality, the approach we took with Java projects may not work for the OSS written in other languages.

\subsection{Future Work}
We hope to expand this work to more programming languages. This would involve understanding Change Distiller?s algorithm and implementing its AST converter to languages like C and Python. We will continue to pursue our high-level goal of understanding the process of bug fixing in OSS. *We will add more future work here*

We hope that our approach ~\cite{Giachino:2014:CRD:2731750.2731754} is not limited to Change Distiller and Naturalness. Our predictive model can be enhanced by analyzing the repository?s pull request history for patterns in acceptable structural changes [reference]. If we are able to take this approach, our predictive model may be able to classify future changes and ultimately future bugs. ~\cite{Yu}

\section*{Acknowledgment}

The authors would like to thank...
more thanks here



\bibliography{myBib}
\bibliographystyle{plain}


% that's all folks
\end{document}


